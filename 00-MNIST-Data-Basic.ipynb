{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST Data Set - Basic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get the MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "# so that the labels are one and coded already for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Alternative sources of the data just in case: **\n",
    "\n",
    "* http://yann.lecun.com/exdb/mnist/\n",
    "* https://github.com/mrgloom/MNIST-dataset-in-different-formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "So if we take a look at the type of what this is it's basically\n",
    "this kind of specialized tensor flow\n",
    "data set than it already has a lot of convenient methods built \n",
    "into it later on when we deal with other\n",
    "data sets we're going to have to basically make our own classes\n",
    "to create these kind of methods.\n",
    "But for now we're going to take advantage of these convenience\n",
    "methods.\n",
    "\"\"\"\n",
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo if you choose one of those sets such as train and\\nthen hit tab here you can see there's images labels.\\nThere's also a next batch function which is nice basically\\nfeeds in batches number of examples epochs\\ncompleted etcetera.\\nSo if we take a look at the images themselves you get the\\noutput of a bunch of arrays.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images\n",
    "\"\"\"\n",
    "So if you choose one of those sets such as train and\n",
    "then hit tab here you can see there's images labels.\n",
    "There's also a next batch function which is nice basically\n",
    "feeds in batches number of examples epochs\n",
    "completed etcetera.\n",
    "So if we take a look at the images themselves you get the\n",
    "output of a bunch of arrays.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So then you can actually ask for how many examples \n",
    "# there are as we discussed there's 55000 training\n",
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.validation.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape\n",
    "# (55000 images by 784 pizels, remmer 28 x 28say this train images and \n",
    "# if I check out the shape of this this is 55000 images by 784 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.23137257, 0.6392157 , 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       0.7607844 , 0.43921572, 0.07058824, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.01568628, 0.5176471 , 0.93725497, 0.9921569 ,\n",
       "       0.9921569 , 0.9921569 , 0.9921569 , 0.9960785 , 0.9921569 ,\n",
       "       0.627451  , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.5372549 ,\n",
       "       0.9921569 , 0.9960785 , 0.9921569 , 0.9921569 , 0.9921569 ,\n",
       "       0.75294125, 0.9960785 , 0.9921569 , 0.8980393 , 0.0509804 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01568628, 0.5372549 , 0.9843138 , 0.9921569 , 0.9568628 ,\n",
       "       0.50980395, 0.19215688, 0.07450981, 0.01960784, 0.6392157 ,\n",
       "       0.9921569 , 0.8235295 , 0.03529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.37254903, 0.9921569 ,\n",
       "       0.9921569 , 0.8431373 , 0.1764706 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.6117647 , 0.9921569 , 0.68235296,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.8431373 , 0.9960785 , 0.8117648 , 0.09019608,\n",
       "       0.        , 0.        , 0.        , 0.03921569, 0.3803922 ,\n",
       "       0.85098046, 0.9176471 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.83921576,\n",
       "       0.9921569 , 0.2784314 , 0.        , 0.        , 0.00784314,\n",
       "       0.19607845, 0.8352942 , 0.9921569 , 0.9960785 , 0.7058824 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.83921576, 0.9921569 , 0.19215688,\n",
       "       0.        , 0.        , 0.19607845, 0.9921569 , 0.9921569 ,\n",
       "       0.9921569 , 0.7176471 , 0.04705883, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.7803922 , 0.9921569 , 0.95294124, 0.7686275 , 0.62352943,\n",
       "       0.95294124, 0.9921569 , 0.9686275 , 0.5411765 , 0.03137255,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.16470589, 0.9921569 ,\n",
       "       0.9921569 , 0.9921569 , 0.9960785 , 0.9921569 , 0.9921569 ,\n",
       "       0.39607847, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.23137257, 0.58431375, 0.9960785 , 0.9960785 , 0.9960785 ,\n",
       "       1.        , 0.9960785 , 0.6862745 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13333334, 0.75294125, 0.9960785 , 0.9921569 ,\n",
       "       0.9921569 , 0.9921569 , 0.7843138 , 0.53333336, 0.89019614,\n",
       "       0.9450981 , 0.27058825, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33333334, 0.9686275 ,\n",
       "       0.9921569 , 0.9960785 , 0.9921569 , 0.77647066, 0.48235297,\n",
       "       0.07058824, 0.        , 0.19607845, 0.9921569 , 0.8352942 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.2784314 , 0.9686275 , 0.9921569 , 0.9294118 , 0.75294125,\n",
       "       0.2784314 , 0.02352941, 0.        , 0.        , 0.        ,\n",
       "       0.00784314, 0.5019608 , 0.9803922 , 0.21176472, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.46274513, 0.9921569 ,\n",
       "       0.8705883 , 0.14117648, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03137255, 0.7176471 ,\n",
       "       0.9921569 , 0.227451  , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.46274513, 0.9960785 , 0.54509807, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05490196, 0.7294118 , 0.9960785 , 0.9960785 , 0.227451  ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.2784314 ,\n",
       "       0.9686275 , 0.9686275 , 0.54509807, 0.0627451 , 0.        ,\n",
       "       0.        , 0.07450981, 0.227451  , 0.87843144, 0.9921569 ,\n",
       "       0.9921569 , 0.8313726 , 0.03529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.42352945, 0.9921569 ,\n",
       "       0.9921569 , 0.92549026, 0.6862745 , 0.6862745 , 0.9686275 ,\n",
       "       0.9921569 , 0.9960785 , 0.9921569 , 0.77647066, 0.16862746,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.2627451 , 0.8352942 , 0.8980393 , 0.9960785 ,\n",
       "       0.9921569 , 0.9921569 , 0.9921569 , 0.9921569 , 0.83921576,\n",
       "       0.48627454, 0.02352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.60784316, 0.60784316, 0.8745099 ,\n",
       "       0.7843138 , 0.46274513, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember that's just 28 times 28. Let's get one of these...\n",
    "mnist.train.images[5]\n",
    "# You get flattened out image, and as usual the shpe is as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.12156864, 0.5176471 ,\n",
       "        0.9960785 , 0.9921569 , 0.9960785 , 0.8352942 , 0.32156864,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08235294, 0.5568628 , 0.91372555, 0.98823535,\n",
       "        0.9921569 , 0.98823535, 0.9921569 , 0.98823535, 0.8745099 ,\n",
       "        0.07843138, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.48235297, 0.9960785 , 0.9921569 , 0.9960785 , 0.9921569 ,\n",
       "        0.87843144, 0.7960785 , 0.7960785 , 0.8745099 , 1.        ,\n",
       "        0.8352942 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.7960785 , 0.9921569 , 0.98823535, 0.9921569 , 0.8313726 ,\n",
       "        0.07843138, 0.        , 0.        , 0.2392157 , 0.9921569 ,\n",
       "        0.98823535, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.16078432,\n",
       "        0.95294124, 0.87843144, 0.7960785 , 0.7176471 , 0.16078432,\n",
       "        0.59607846, 0.11764707, 0.        , 0.        , 1.        ,\n",
       "        0.9921569 , 0.40000004, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.15686275, 0.07843138, 0.        , 0.        , 0.40000004,\n",
       "        0.9921569 , 0.19607845, 0.        , 0.32156864, 0.9921569 ,\n",
       "        0.98823535, 0.07843138, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.32156864,\n",
       "        0.83921576, 0.12156864, 0.4431373 , 0.91372555, 0.9960785 ,\n",
       "        0.91372555, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24313727, 0.40000004, 0.32156864, 0.16078432,\n",
       "        0.9921569 , 0.909804  , 0.9921569 , 0.98823535, 0.91372555,\n",
       "        0.19607845, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.59607846, 0.9921569 , 0.9960785 , 0.9921569 ,\n",
       "        0.9960785 , 0.9921569 , 0.9960785 , 0.91372555, 0.48235297,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.59607846, 0.98823535, 0.9921569 , 0.98823535,\n",
       "        0.9921569 , 0.98823535, 0.75294125, 0.19607845, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.24313727, 0.7176471 , 0.7960785 , 0.95294124,\n",
       "        0.9960785 , 0.9921569 , 0.24313727, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.15686275,\n",
       "        0.6745098 , 0.98823535, 0.7960785 , 0.07843138, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.08235294, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.7176471 , 0.9960785 , 0.43921572, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24313727, 0.7960785 , 0.6392157 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2392157 , 0.9921569 , 0.5921569 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
       "        0.83921576, 0.75294125, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04313726, 0.8352942 , 0.9960785 , 0.5921569 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.40000004,\n",
       "        0.9921569 , 0.5921569 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.16078432,\n",
       "        0.8352942 , 0.98823535, 0.9921569 , 0.43529415, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.16078432,\n",
       "        1.        , 0.8352942 , 0.36078432, 0.20000002, 0.        ,\n",
       "        0.        , 0.12156864, 0.36078432, 0.6784314 , 0.9921569 ,\n",
       "        0.9960785 , 0.9921569 , 0.5568628 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.6745098 , 0.98823535, 0.9921569 , 0.98823535, 0.7960785 ,\n",
       "        0.7960785 , 0.91372555, 0.98823535, 0.9921569 , 0.98823535,\n",
       "        0.9921569 , 0.50980395, 0.07843138, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08235294, 0.7960785 , 1.        , 0.9921569 , 0.9960785 ,\n",
       "        0.9921569 , 0.9960785 , 0.9921569 , 0.9568628 , 0.7960785 ,\n",
       "        0.32156864, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07843138, 0.5921569 , 0.5921569 , 0.9921569 ,\n",
       "        0.67058825, 0.5921569 , 0.5921569 , 0.15686275, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[1].reshape(28, 28)\n",
    "# do you see the brackets? \n",
    "# This means we can show this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_img = mnist.train.images[1].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c35e51710>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c35e369e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(my_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b511940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b517780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape(28,28), cmap=\"gist_gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data is already normalized for you.\n",
    "\n",
    "Can you find out how?\n",
    "\n",
    "Answer:\n",
    "\n",
    "```python\n",
    "my_img.min()\n",
    "my_img.max()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12bf13ef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADQAAAD8CAYAAAA4w4cyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACMdJREFUeJztnW2MFVcZx39/2BdgWViQglAaWYWgQJu2IcZGjCT1pa1a\nTEwMxsR+qBITTdpPCvaT8YvW2PipJqRWMWobrNQSgm2gaYImxhYqRd62bEuxvBRKkdJSWJby+GHO\n1rm33btz955799nh/JLJPXNm5sz8d+aenfs/zzwjM6NMTBjrA4hNEuSdJMg7SVBRJN0mqU9Sv6S1\nzdrP+zCz6BMwEXgJ+CjQAbwALGnGvqqnZp2hTwL9ZvaymV0CHgVWNWlfFTRL0LXAq7n5o6HuPSSt\nkbRT0s42tZmk12PseMw6BTNbb2bLzWz5FKYBHInRbrMEHQOuy83PD3VNp1mCngMWSeqV1AGsBjY3\naV8VtDWjUTO7LOn7wFNkPd7DZravGfuqpimCAMxsK7C1We0PR7pT8E4S5J0kyDtJkHeSIO8kQd5J\ngryTBHknCfLO1SdI0sOSTknam6ubKWmbpEPhc0Zu2brgZ/dJ+mKzDnw4ipyh3wK3VdWtBZ42s0XA\n02EeSUvILKulYZsHJU2MdrQFGFGQme0AzlRVrwI2hPIG4Ku5+kfNbMDMDgP9ZD53yxjtd2iOmZ0I\n5deAOaE8oqc9RN7bHmRglIfxfhruFCwbP6k7NiDvbbfT2ehhvMdoBZ2UNBcgfJ4K9WPmaQ8xWkGb\ngbtC+S7giVz9akmdknqBRcCzjR1inRQYjXsEOAEMkn0n7gY+RNa7HQK2AzNz699HNnrXB9xeZNSt\nmxkG7IwxgicPoTHTNNPe4r+7zGx5o21dfXcK440kyDtJkHeSIO8kQc1g0Q3no7XlQtChPV3R2nIh\nKCZJkHeSIO+4EDTv+pJ1212K15YLQf0HeqK1VcTbvk7SM5L2S9on6Z5QH83ftsHBxlRUNDay6zMX\nuDmUu4EXgSXA/cDaUL8W+FkoLyGL0+4EeskcoImtcn2KeNsnzOz5UH4LOEBm77r0t+v6DklaANwE\n/JMG/e0x97YlTQX+DNxrZufyy0bjb+e97SvzZ9azaU0KCZLUTibmD2a2KVRH87eXzTxVa3FdFOnl\nBPwaOGBmD+QWRfO3j1+eUu9xD0+BXm4F2eW0B9gdpjuI6G8nb7sGLu4UYpIEeceFoNK5Pi29224F\nV6a0R2vLhSC9+U60tlwIikkS5J0kqBl0LYl3P+lC0JkL8X4+uBB0ffcb0dpyIejFPSU7QzFJgrxT\nxCSZJOlZSS8EK/jHod5nqHMBk0TA1FBuJzMZP8U4toLNzN4Os+1hMmJawd0t7uUkTZS0m8xM3GZm\nDVvBeRb2RsneARRMP2Bm7wI3SuoBHpe0rGq5Sarr/kXSGmANwCSmAJfq2XxY6urlzOws8AzZYwAN\nWcE2VnHbkq4JZwZJk4HPAwdxGupc5JKbC2wID2VMADaa2RZJ/wA2SrqbLOPL1wHMbJ+kjcB+4DLw\nvXDJtoRkBXvHhaBL80oWjdVxvGTOaUySIO8kQc3gSk/JPIUJgyXz5RZ+7HS0tlwI6k9hzsOTBHkn\nCWoGA72To7XlQtCy7pJ123vPzI7WlgtBnUfH4PdQMBv/JWlLmHfpbddzhu4hiwgewmUaj6JW8Hzg\nS8BDuepxHeb8S+AHwJVc3fgMc5b0ZeCUme0abp1Gw5w/ccPlejatSRHn9NPAnZLuACYB0yT9nuBt\nm9kJV2k86hlMAlYCW0L551QOeN0fykupHPB6mRYOeDWSzfmnJG/7g0nedg2SIO+4EFS68LJ5bSUL\nok1P69cgCfJOEuQdH4KmlMyX61pQsv9D5/fHyz/gQlBMkiDvJEHeKeqcviLp35J2S9oZ6lx620Xt\nq1eAWVV10eK2O+fNb13cdg2iedtjEV5mwHZJu0J4Mjj1tosajSvM7Jik2cA2SQfzC0cTt21m64H1\nkPlyF4lz+1PoDJnZsfB5Cnic7BJymaK6yOhDl6TuoTLwBWAv4zhuew7Z4wFD6//RzJ6U9BzJ2/5g\nkrddgyTIO0lQM+hZGm/Q2IWgs/vivefWhaCYuBCULrkauBAUkyTIOy4ElS6ItvPwhWhtuRAUExeC\nSpeOLQ3r1+DqFCSpR9Jjkg5KOiDplvHubW8Avh3KHUAPEb3tybPjedtFxEwHDhMcolx9HzA3lOcC\nfaG8DliXW+8p4JZa+2hpkpXwV34d+E14VOChYDhG87Z7rj1XvXjUFBHUBtwM/MrMbgLOEx4LGMKy\nUzHquO2TbR+uZ9OaFBF0FDhqWaYYgMfIBEbztjuPtDBOwcxeA16VtDhU3Upm87r0tov2cjcCO8lS\nVP8FmEFKTz08pfO2S3dzej7iReJCUOnenZJ+PtQgCfKOC0HqKFkC8YUfPxutLReCYuJC0IHjJXu4\nve10yW59YpIEeScJagal+z10qH9WtLaKBAAuDuHNQ9M5SfdGtYLfiTfgVZ/NChPJTMWPMF7TU1dx\nK/CSmR1hnKfwGGI18EgoR0tPHZN60uB0AHcCf6peNhoreMxykuS4HXjezE6G+fGZnjrHN/j/5Qbj\n3AruAt4ApufqkhU8HKWzgmOSBHknCfJOEuSdJMg7SZB3kiDvJEHeSYK8UzpBLn6CS7oIDJpZd6Nt\neTlD75IZKg3jRVA0Sico3tN8jbEJ+FuMhlx0CjEp3SWXBDWCpPskXZI0KOl09XCmpJWSBsJ0IQx/\nVmR8GpEYBnlBw78dGAQ+CzwQyl+hcjjzh2TDMiJ7K+8AVRmfRtxPCwV9Bzgdyn1kr0F9ksonWzaT\njUENbTMILK1nP6285BYDQy9JmRNEzaNyOPMaYKGkPZL+SnYHsakq41NNmvZ/SNJ2IP/YyRygW9Kq\n/HpmFVmb3gS+ZmbbQtL/lcA3gf8QMj6Z2Y5a+23aGTKzz5nZsqEJ+BHwtpk9AZwkO2PHq4YzjwCz\nwvZbyf7gF6wy41NNWnnJ/Q6YLukzwFZgBfAglcOZO4BvKeO7ZC/O2F+V8akmLbv1MbMBST8hG8YU\ncA74BTAV+HtYbQFZLMRFMjEngd3kMj6NtJ906+OdJMg7SZB3kiDv/A/Vf/H9kA72HwAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b4928d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape(784,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12c2faac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD4AAAD8CAYAAAAv4Rf7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b79ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape(784,1),cmap='gist_gray',aspect=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So now that we understand our data again it's just an array of images that are all flattened out and\n",
    "you could reshape that to visualize it.\n",
    "- However we won't be reshaping for training purposes.\n",
    "- We're going to do in the very next lecture is actually create the model.\n",
    "\n",
    "\n",
    "## Create the Model\n",
    "\n",
    "So again we're doing a very basic soft Max regression approach.\n",
    "\n",
    "So I'm going to map out the steps we need to do here.\n",
    "\n",
    "\n",
    "1. **PLACEHOLDERS**: We need to create our **placeholders** so we'll have our placeholders which essentially going to be X then\n",
    "2. **VARIABLES**: we also need our **variable's** then \n",
    "3. **CREATE GRAPH OPERATIONS**: we're going to need to create the actual **graph operations** then we'll\n",
    "4. **LOSS FUNCTION**: need to have our Loss function then \n",
    "5. **OPTIMIZER**: we'll need our optimizer how we're going to optimize that last function. Essentially what we're going to minimize and \n",
    "4. **CREATE & RUN SESSION**: then we'll create our session and run all this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "So our placeholder we only have one set of input \n",
    "which is our image data. So I'll say T.F. placeholder\n",
    "and it's going to be float32 and we're going to indicate\n",
    "that the shape\n",
    "is none because it's essentially going to be filled in by\n",
    "the batch size.\n",
    "And then 784 because that's 28 times 28 remember we have\n",
    "flattened arrays.\n",
    "So thats our placeholder\n",
    "\"\"\"\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"And then we have our variables which is going to be our weights \n",
    "and our bias.\n",
    "So we'll have our weights here.\n",
    "And this is going to be T.F. variable.\n",
    "And then just go ahead and initialize this our weights and biases with zeros.\n",
    "We already have a discussion of why this is probably not such a great idea.\n",
    "But right now we want to keep things as simple as possible.\n",
    "So we'll initiate weights & biases as zeros.\n",
    "Now typically you probably shouldn't do this but again this is just \n",
    "for simplification.\n",
    "\n",
    "So our weight should be 784 by 10 because we have 784 pixels by \n",
    "10 possible labels.\n",
    "10 because 0-9 possible numbers\n",
    "\"\"\"\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So that just needs to be the same length as the actual labels.\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\"\"\"So those are variables you can mess around and maybe add \n",
    "some random \n",
    "numbers here instead of zeros in fact tensor float does \n",
    "have its own.\n",
    "If you take a look at random here it has its own random normal and \n",
    "other things of that nature. But again keeping things simple \n",
    "We'll do it all zeros.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the Graph\n",
    "Now we need to create our graph operation.\n",
    "So remember this is what we just basically \n",
    "went over in those slides trust.\n",
    "If we just have a matrix multiplication between X and W \n",
    "and then we're going to add to it.\"\"\"\n",
    "\n",
    "y = tf.matmul(x,W) + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimizer\n",
    "\n",
    "Then we need to give our loss function.\n",
    "So in order to have a loss function we need one more placeholder and that is the y_true values.\n",
    "\n",
    "Flo 32 and then this is going to be **none** because it's the again the batch size size and then 10 because there's 10 possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Cross Entropy\n",
    "So we still need to add more for this loss function here.\n",
    "So we're going to use a cross entropy loss function very common loss function \n",
    "to use say cross entropy = then we can basically directly import this from sensor flow we'll say tf.nn.softmax_cross_with_logit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "                               (labels=y_true, logits=y))\n",
    "# this is basically just tensor flows built in across\n",
    "# entropy loss function where you're basically passing in.\n",
    "# You say, these are labels = my true values, where you pass in y\n",
    "# which will be your actual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's create our **optimizer** can zoom back in for this will say that our optimizer= tf.train...\n",
    "And let's go ahead and just say `learning rates of 0.5` a little bit of a larger learning rate and you can play around that later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To minimize our loss function\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now it's time to create our session remember whenever we're creating \n",
    "# a session we need to initialize all the global variables.\n",
    "# So we'll do the following.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9227\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Train the model for 1000 steps on the training set\n",
    "    # Using built in batch feeder from mnist for convenience\n",
    "    \n",
    "    for step in range(1000):\n",
    "        \"\"\"        \n",
    "        So we'll say for step in range and we'll do 1000 steps on \n",
    "        the training set. So if we take a look at MNIST that train \n",
    "        the next batch this is the method where you just need to \n",
    "        provide then the batch size and it will return a \n",
    "        batch of training samples.So we'll say we want 100 for \n",
    "        our batch size.\n",
    "        \"\"\"\n",
    "        batch_x , batch_y = mnist.train.next_batch(100) \n",
    "        \"\"\"        \n",
    "        So this is essentially tuple unpacking packing and \n",
    "        if you read the documentation of what this actually returns \n",
    "        it just returns a tuple with your X and Y.\n",
    "        \"\"\"\n",
    "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y})\n",
    "    \"\"\"\n",
    "    The final step when you perform in this session is to actually \n",
    "    evaluate our model.\n",
    "    So first we need to figure out where we actually predicted the \n",
    "    correct label and \n",
    "    we can use tensor Flo's version of argmax to do that.\n",
    "    Remember arke Max is just a useful function which gives you the\n",
    "    index position \n",
    "    of the highest entry point and with tensor flow it gives you the\n",
    "    highest entry \n",
    "    point of a tensor along some axis. So let's go ahead and show \n",
    "    you how to do that.\n",
    "    And the second argument one is just telling it to do this all \n",
    "    on access equal to one.\n",
    "    \n",
    "    This is going to return the index position of the label with \n",
    "    the highest probability \n",
    "    essentially just saying what label it thinks it is \n",
    "    and then what we want to do is compare this to the actual true values.\n",
    "    You pass these in tf.equal()\n",
    "    One and I want to check where these are equal to so tensor flow \n",
    "    has a nice equal function where I can pass these two tensors in \n",
    "    and it basically reports back a list of booleans.\n",
    "    So the result of this correct predictions looks like\n",
    "    Result would look like [True, False, True...]\n",
    "    So once again tf.argmax(y,1) outputs the predicted label \n",
    "    because of the way we \n",
    "    put the one_hot encoding as well as using the softmac regression.\n",
    "    tf.argmax(y_true,1) = is same, execpt this is the true \n",
    "    values/ ground truth labels\n",
    "    and we check where they are equal to each other seentially outputting\n",
    "    the list of booleans.\n",
    "    \n",
    "    next we need to cast the booleans into floating points \n",
    "    [True, False, True...] --> [1, 0, 1, 1,..]\n",
    "    \"\"\"    \n",
    "    # Evaluate Test the Train Model\n",
    "    prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
    "    \n",
    "    \"\"\"\n",
    "    So then we can actually run this.\n",
    "    So say print the result of a session run and I'm going \n",
    "    to run ack because \n",
    "    this accuracy is essentially its own graph.\n",
    "    So that's similar to what we did appear as far as the fighting steps the \n",
    "    take except now we're just actually running accuracy.\n",
    "\n",
    "   And then it needs its own feed dictionary so the feed dictionary\n",
    "   we're going \n",
    "   to feed in this time is the actual test set.\n",
    "   So we'll say x is equal to just test images. So those 10000 images\n",
    "   and then \n",
    "   we'll say why true is equal to minus test labels. So again \n",
    "   pretty convenient \n",
    "   that we have those methods and attributes the call but that's what we're doing\n",
    "   here to evaluate our model.\n",
    "   We have this correct prediction which just turns the actual \n",
    "   predicted labels and \n",
    "   true labels into a list of true or false booleans and then we \n",
    "   convert that to ones and zeroes.\n",
    "   Take the mean of that are accuracy. So we're going to run that \n",
    "   accuracy in feet in our data.\n",
    "\n",
    "   Where are the test images and why true is this test labels.\n",
    "   Now let's run that running this for a thousand steps.\n",
    "   It looks like we get 91 percent accuracy.\n",
    "   Now you may be thinking daar That's pretty good.\n",
    "   A 91 percent accuracy on handwritten digits that only \n",
    "   took a few seconds on this computer.\n",
    "   Not bad.\n",
    "   Well in fact if you compare this to some of the latest \n",
    "   models this actually isn't that great.\n",
    "   In fact some of the very best models can get over ninety nine\n",
    "   point seven percent accuracy.\n",
    "   So what we're going to learn now later on is how we can use convolutional \n",
    "   Nero that works to perform much much better than just a high or low 90s.\n",
    "   We're going to get up to the high 90s.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(sess.run(acc,feed_dict={x:mnist.test.images,\n",
    "                                  y_true:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the last 2 lines as they are pretty useful to understand\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#### PREDICTED\n",
    "\n",
    "- [3,4] TRUE [3, 9]\n",
    "- tf.equal says\n",
    "- [True, False]\n",
    "- Next step us to cast into 1s and 0s.\n",
    "-  [1.0, 0.0]\n",
    "-  acciracy = avg of the two which is 0.5, 50%\n",
    " \n",
    " **Long explanation**\n",
    " \n",
    " When we ask for argmaxit just returns back to sort of list of the actual numbers it predicted for\n",
    " the classes it says and I think the first 20 examples three in the second one is four.\n",
    "And then we say well the real values were three and nine. Then when we asked for a tf.equal then the result of that is something like this.\n",
    "We say true and false because here we can see that they're equal that first one is equal.\n",
    "The second one however is not equal for is not equal to nine. So then that is a correct prediction.\n",
    "\n",
    "True false.\n",
    "\n",
    "The next step after that is to cast that correct prediction into T.F. of that flow essentially casting it to ones and zeroes. So then we get a list that looks like this one point zero and zero point zero then the final step is to reduce the mean essentially take the average of this. And if you take the average of that well that gives you back a single number 0.5 and that number directly relates to the percentage that you got.\n",
    "\n",
    "Correct.\n",
    "And that's because these are ones and zeros.\n",
    "So if we take the average of one and zero.\n",
    "So 1 divided by two is there a point five which directly results into 50 percent correct and that'sher accuracy.\n",
    "So just these two lines are doing what are essentially almost like four steps here.\n",
    "\n",
    "\n",
    "So we compare the predicted the true values we get back a list of matches we convert that list of matches to actual numbers ones and zeros. And because they're ones and zeros it means directly through math. If we take the average of those we get back the percent that we got right. Or the accuracy. So in this really simple example we've got 50 percent right out of two samples inputted compared to there two true labels.\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    " \n",
    "\n",
    "\n",
    "While this may seem pretty good, we can actually do much better, the best models can get above 99% accuracy.\n",
    "\n",
    "How do they do this? By using other models, such as convolutional neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

